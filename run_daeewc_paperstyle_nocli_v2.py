
# -*- coding: utf-8 -*-
"""
DAEWC paper-style experiment (NO CLI args; config is inside this file)

Goals:
- Cross-domain fake news detection: Political (source) -> Medical (target)
- Few-shot adaptation (k-shot PER CLASS) and continual-learning stability
- Domain-aware adapters + backbone-only EWC (DAEWC)
- Record target Macro-F1, source forgetting (ΔMacro-F1), trainable params, and wall-clock time.

Notes:
- This script uses a SIMPLE CNN backbone for speed. If you want Transformer, the same protocol applies.
- We use a 3-way split per domain: train / dev / test.
  - dev: threshold calibration (and optional early stopping)
  - test: reporting
- We keep a domain-specific head (multi-head), matching your paper text "task head for that domain".
  This avoids "fake forgetting" caused purely by a shared head being overwritten.

Author: (generated by ChatGPT)
"""

import os
import random
import time
import gc
import re
import hashlib
from dataclasses import dataclass
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
import tensorflow as tf

from tensorflow.keras import Model
from tensorflow.keras import layers as L
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
)

# =========================================================
# 0. GPU + Reproducibility
# =========================================================

def setup_gpu():
    gpus = tf.config.list_physical_devices("GPU")
    if gpus:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
        except Exception as e:
            print("GPU memory growth set failed:", e)

def reset_seeds(seed: int):
    os.environ["PYTHONHASHSEED"] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    tf.random.set_seed(seed)

def clear_tf():
    tf.keras.backend.clear_session()
    gc.collect()

setup_gpu()


# =========================================================
# 1. Config (EDIT HERE)
# =========================================================

# ---------- data paths ----------
PATH_FAKE = "./news/Fake.csv"
PATH_REAL = "./news/True.csv"
PATH_COVID_FAKE = "./covid/fakeNews.csv"
PATH_COVID_REAL = "./covid/trueNews.csv"

# ---------- experiment seeds ----------
# If you want 5 seeds for CI: [42, 43, 44, 45, 46]
SEEDS = [42, 43, 44, 45, 46]

# Splits are fixed (do not change across runs) to make seed variance reflect init/optimization
DATA_SPLIT_SEED = 123

# Few-shot setting: k-shot per class (binary => 2k total)
SHOTS_PER_CLASS_LIST = [10, 20, 80, 160]

# If you prefer fraction mode like your old script, set USE_FRACTIONS=True and edit FRACTIONS.
USE_FRACTIONS = False
FRACTIONS = [0.01, 0.05, 0.10, 0.20, 0.50, 1.00]

# ---------- preprocessing ----------
MIN_CHAR_LEN = 40
LOWERCASE = True

# Tokenizer
MAX_NUM_WORDS = 5000
MAXLEN = 1000
TOKENIZER_MODE = "source"  # "source" (paper-strict) OR "union" (source_train + target_train texts)
DEDUP_GROUP_SPLIT = True   # group by (normalized) text hash to avoid exact-duplicate leakage across splits
KSHOT_UNIQUE_TEXT = True   # sample few-shot examples from UNIQUE text hashes (harder, more realistic)

# ---------- model ----------
EMBED_DIM = 100
CONV_FILTERS = 128
KERNEL_SIZE = 5
DENSE_UNITS = 128
DROPOUT = 0.5

# Adapter / Gate
ADAPTER_R = 16
GATE_HIDDEN = 64
NUM_DOMAINS = 2  # 0=political, 1=medical

# ---------- training ----------
EPOCHS_SOURCE = 5
EPOCHS_TARGET_STAGE1 = 3   # adapters/head only
EPOCHS_TARGET_STAGE2 = 3   # + small backbone update
BATCH_SIZE = 64

LR_SOURCE = 1e-3
LR_TARGET = 1e-3

# If stage2 unfreezes backbone, gradients on backbone are scaled by this factor (<1 is safer)
BACKBONE_LR_MULT = 0.5

# Optional early stopping on DEV (set 0 to disable)
EARLY_STOP_PATIENCE = 0

# ---------- EWC ----------
USE_EWC = True
EWC_LAMBDA = 20.0          # base strength
EWC_AUTO_SCALE = True      # scale lambda by fisher magnitude so Plain vs Adapter are comparable
EWC_EPS = 1e-12

PROX_ALPHA = 0.0           # optional proximity term (L2 to source weights); set e.g. 0.1

# Fisher estimation
FISHER_SAMPLES = 2000
FISHER_BATCH_SIZE = 64
FISHER_LABEL_SMOOTH = 0.10
FISHER_TRAINING_TRUE = True

# ---------- evaluation ----------
# Calibrate threshold on DEV to maximize Macro-F1 (recommended to avoid the "thr=0.5 collapse")
CALIBRATE_THR_ON_DEV = True
THR_GRID_SIZE = 201         # number of thresholds scanned between 0..1
REPORT_WITH_FIXED_THR_05 = False  # optionally also report thr=0.5 metrics

# ---------- baselines toggles ----------
INCLUDE_SCRATCH = True
INCLUDE_JOINT_REPLAY_UPPER = True
INCLUDE_PLAIN_EWC = True
INCLUDE_ADAPTER_ONLY = True
INCLUDE_DAEWC = True

# Outputs
OUT_RAW_CSV = "results_daeewc_paperstyle_v2_raw.csv"
OUT_SUMMARY_CSV = "results_daeewc_paperstyle_v2_summary.csv"


# =========================================================
# 2. Utilities
# =========================================================

def basic_clean(s: str) -> str:
    s = "" if s is None else str(s)
    s = s.strip()
    if LOWERCASE:
        s = s.lower()
    return s

def build_tokenizer(texts: List[str], num_words: int) -> Tuple[Tokenizer, int]:
    tok = Tokenizer(num_words=num_words, oov_token="<OOV>")
    tok.fit_on_texts(texts)
    vocab_size = min(num_words, len(tok.word_index) + 1)
    return tok, vocab_size

def vectorize(tok: Tokenizer, texts: List[str], maxlen: int) -> np.ndarray:
    seqs = tok.texts_to_sequences(texts)
    return pad_sequences(seqs, maxlen=maxlen)

def domain_ids(n: int, domain: int) -> np.ndarray:
    return np.full((n,), domain, dtype=np.int32)

def count_trainable_params(model: tf.keras.Model) -> int:
    return int(np.sum([np.prod(v.shape) for v in model.trainable_variables]))

def bce_value(y_true: np.ndarray, y_prob: np.ndarray) -> float:
    y_true = np.asarray(y_true).astype(np.float32).reshape(-1, 1)
    y_prob = np.asarray(y_prob).astype(np.float32).reshape(-1, 1)
    loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)
    return float(loss_fn(y_true, y_prob).numpy())

def pick_text_col(df: pd.DataFrame) -> str:
    for c in ["Text", "text", "content", "Content"]:
        if c in df.columns:
            return c
    raise ValueError(f"Cannot find text column. Columns={list(df.columns)}")


# ---------- Dedup / leakage controls ----------
def _normalize_for_hash(text: str) -> str:
    """Normalize text for hashing (very lightweight).
    We only use this for *dedup/grouping*, NOT for model input.
    """
    t = str(text).lower()
    t = re.sub(r"https?://\S+|www\.\S+", " ", t)
    t = re.sub(r"@\w+", " ", t)
    t = re.sub(r"[^a-z0-9\s]+", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def _text_hash(text: str) -> str:
    return hashlib.md5(_normalize_for_hash(text).encode("utf-8")).hexdigest()

def _group_keys(texts: List[str]) -> List[str]:
    return [_text_hash(t) for t in texts]


def split_train_dev_test(
    texts: List[str],
    labels: np.ndarray,
    seed: int,
    test_size: float = TEST_SIZE,
    dev_size: float = DEV_SIZE
):
    """Split into train/dev/test.

    Default behavior is **grouped split** by exact-duplicate hashes (DEDUP_GROUP_SPLIT=True),
    so the same (or trivially normalized) text will never appear in multiple splits.
    This is important in few-shot settings: otherwise a single duplicated item can inflate test F1.
    """
    labels = np.asarray(labels).astype(int)
    texts = list(texts)

    if not DEDUP_GROUP_SPLIT:
        X_train, X_temp, y_train, y_temp = train_test_split(
            texts, labels, test_size=(test_size + dev_size), random_state=seed, stratify=labels
        )
        dev_ratio = dev_size / (test_size + dev_size)
        X_dev, X_test, y_dev, y_test = train_test_split(
            X_temp, y_temp, test_size=(test_size / (test_size + dev_size)),
            random_state=seed, stratify=y_temp
        )
        return X_train, X_dev, X_test, y_train, y_dev, y_test

    # --- grouped split by normalized hash key ---
    keys = _group_keys(texts)
    # map: key -> list(indices)
    key2idx = {}
    for i, k in enumerate(keys):
        key2idx.setdefault(k, []).append(i)

    # each group gets a representative label (majority)
    group_keys = []
    group_labels = []
    for k, idxs in key2idx.items():
        labs = labels[idxs]
        vals, cnts = np.unique(labs, return_counts=True)
        lab = int(vals[int(np.argmax(cnts))])
        group_keys.append(k)
        group_labels.append(lab)

    group_keys = np.array(group_keys)
    group_labels = np.array(group_labels)

    # split groups, stratified by group label
    g_train, g_temp, y_g_train, y_g_temp = train_test_split(
        group_keys, group_labels, test_size=(test_size + dev_size),
        random_state=seed, stratify=group_labels
    )
    g_dev, g_test, y_g_dev, y_g_test = train_test_split(
        g_temp, y_g_temp, test_size=(test_size / (test_size + dev_size)),
        random_state=seed, stratify=y_g_temp
    )

    def expand(g_keys: np.ndarray):
        idxs = []
        for k in g_keys:
            idxs.extend(key2idx[str(k)])
        return idxs

    train_idx = expand(g_train)
    dev_idx = expand(g_dev)
    test_idx = expand(g_test)

    rng = np.random.RandomState(seed)
    rng.shuffle(train_idx)
    rng.shuffle(dev_idx)
    rng.shuffle(test_idx)

    X_train = [texts[i] for i in train_idx]
    X_dev = [texts[i] for i in dev_idx]
    X_test = [texts[i] for i in test_idx]
    y_train = labels[train_idx]
    y_dev = labels[dev_idx]
    y_test = labels[test_idx]
    return X_train, X_dev, X_test, y_train, y_dev, y_test

def sample_k_shot_per_class(
    X_texts: List[str], y: np.ndarray, k: int, seed: int
) -> Tuple[List[str], np.ndarray]:
    """Balanced k-shot *per class* for binary labels {0,1}.

    If KSHOT_UNIQUE_TEXT=True, we sample from **unique (normalized) text hashes**,
    so the 2k few-shot set contains no exact duplicates. This makes few-shot evaluation
    much less prone to accidental leakage/easy memorization.
    """
    y = np.asarray(y).astype(int)
    rng = np.random.RandomState(seed)

    selected = []
    for cls in [0, 1]:
        idx = np.where(y == cls)[0]
        if len(idx) < k:
            raise ValueError(f"Not enough samples for k-shot. Need k={k}, got n_cls={len(idx)} (cls={cls})")

        if not KSHOT_UNIQUE_TEXT:
            chosen = rng.choice(idx, size=k, replace=False)
            selected.extend(chosen.tolist())
            continue

        # group by normalized hash to avoid duplicates
        groups = {}
        for i in idx:
            h = _text_hash(X_texts[i])
            groups.setdefault(h, []).append(i)

        keys = list(groups.keys())
        if len(keys) < k:
            raise ValueError(
                f"Not enough UNIQUE texts for k-shot. Need k={k}, got unique={len(keys)} (cls={cls}). "
                f"Set KSHOT_UNIQUE_TEXT=False to disable, or reduce k."
            )

        rng.shuffle(keys)
        keys = keys[:k]
        for h in keys:
            selected.append(int(rng.choice(groups[h])))

    selected = np.array(selected, dtype=int)
    rng.shuffle(selected)
    X_sub = [X_texts[i] for i in selected]
    y_sub = y[selected]
    return X_sub, y_sub

def stratified_fraction_subset(
    X_texts: List[str], y: np.ndarray, frac: float, seed: int
) -> Tuple[List[str], np.ndarray]:
    if frac >= 1.0:
        return list(X_texts), np.array(y)
    X_sub, _, y_sub, _ = train_test_split(
        X_texts, y, train_size=frac, random_state=seed, stratify=y
    )
    return list(X_sub), np.array(y_sub)

def timed_fit(
    model: tf.keras.Model,
    train_inputs,
    train_y,
    dev_inputs,
    dev_y,
    epochs: int,
    batch_size: int,
    verbose: int = 0,
) -> float:
    callbacks = []
    if EARLY_STOP_PATIENCE and EARLY_STOP_PATIENCE > 0:
        callbacks.append(tf.keras.callbacks.EarlyStopping(
            monitor="val_loss",
            patience=EARLY_STOP_PATIENCE,
            restore_best_weights=True,
        ))
    t0 = time.time()
    model.fit(
        train_inputs, train_y,
        validation_data=(dev_inputs, dev_y),
        epochs=epochs,
        batch_size=batch_size,
        verbose=verbose,
        callbacks=callbacks,
    )
    return time.time() - t0


# =========================================================
# 3. Metrics + Threshold calibration
# =========================================================

def select_threshold_macro_f1(y_true: np.ndarray, y_prob: np.ndarray, grid_size: int = THR_GRID_SIZE) -> float:
    """
    Choose threshold that maximizes Macro-F1 on a DEV set.
    """
    y_true = np.asarray(y_true).astype(int)
    y_prob = np.asarray(y_prob).ravel()

    thresholds = np.linspace(0.0, 1.0, grid_size)
    best_thr = 0.5
    best_f1 = -1.0

    for thr in thresholds:
        y_pred = (y_prob >= thr).astype(int)
        f1m = f1_score(y_true, y_pred, average="macro", zero_division=0)
        if f1m > best_f1:
            best_f1 = f1m
            best_thr = float(thr)
    return best_thr

def evaluate_with_threshold(y_true: np.ndarray, y_prob: np.ndarray, thr: float) -> Dict[str, float]:
    y_true = np.asarray(y_true).astype(int)
    y_prob = np.asarray(y_prob).ravel()
    y_pred = (y_prob >= thr).astype(int)

    acc = accuracy_score(y_true, y_pred)
    prec_macro = precision_score(y_true, y_pred, average="macro", zero_division=0)
    rec_macro = recall_score(y_true, y_pred, average="macro", zero_division=0)
    f1_macro = f1_score(y_true, y_pred, average="macro", zero_division=0)
    bce = bce_value(y_true, y_prob)

    return {
        "thr": float(thr),
        "acc": float(acc),
        "prec_macro": float(prec_macro),
        "rec_macro": float(rec_macro),
        "f1_macro": float(f1_macro),
        "bce": float(bce),
    }

def pretty_print_metrics(prefix: str, met: Dict[str, float]):
    print(
        f"{prefix} | thr={met['thr']:.3f}  "
        f"Acc={met['acc']:.4f}  MacroF1={met['f1_macro']:.4f}  BCE={met['bce']:.4f}"
    )


# =========================================================
# 4. Models (multi-head)
# =========================================================

def make_optimizer(lr: float) -> tf.keras.optimizers.Optimizer:
    return tf.keras.optimizers.Adam(learning_rate=lr, clipnorm=1.0)

def build_plain_cnn_mh(vocab_size: int) -> Model:
    """
    Plain CNN with domain-specific heads:
      - shared: emb, conv, shared_fc
      - heads: head0 (political), head1 (medical)
    """
    tok_in = L.Input(shape=(MAXLEN,), dtype="int32", name="tokens")
    dom_in = L.Input(shape=(), dtype="int32", name="domain_id")  # 0 or 1

    x = L.Embedding(vocab_size, EMBED_DIM, name="emb")(tok_in)
    h = L.Conv1D(CONV_FILTERS, KERNEL_SIZE, activation="relu", name="conv")(x)
    pooled = L.GlobalMaxPooling1D(name="gmp")(h)

    z = L.Dense(DENSE_UNITS, activation="relu", name="shared_fc")(pooled)
    z = L.Dropout(DROPOUT, name="drop")(z)

    logit0 = L.Dense(1, activation=None, name="head0")(z)
    logit1 = L.Dense(1, activation=None, name="head1")(z)

    domf = L.Lambda(lambda d: tf.cast(tf.reshape(d, (-1, 1)), tf.float32), name="dom_float")(dom_in)
    logit = L.Lambda(lambda t: (1.0 - t[0]) * t[1] + t[0] * t[2], name="select_head")([domf, logit0, logit1])

    out = L.Activation("sigmoid", name="out")(logit)

    model = Model(inputs={"tokens": tok_in, "domain_id": dom_in}, outputs=out, name="PlainCNN_MH")
    model.compile(optimizer=make_optimizer(LR_SOURCE), loss="binary_crossentropy")
    return model

def build_daeewc_cnn_mh(vocab_size: int) -> Model:
    """
    CNN with domain-aware adapters + gate + domain-specific heads.

    Domain-specific modules:
      - adapter0_* and adapter1_* (per-domain adapters)
      - head0/head1
      - dom_emb table (row 0 and row 1)

    Shared backbone:
      - emb, conv, shared_fc
    Gate MLP weights (dom_proj*, stat_proj) are treated as "adapter side" here; we will freeze them during target adaptation
    to avoid source interference, and rely on domain embedding + adapters for target specialization.
    """
    tok_in = L.Input(shape=(MAXLEN,), dtype="int32", name="tokens")
    dom_in = L.Input(shape=(), dtype="int32", name="domain_id")  # 0 or 1

    x = L.Embedding(vocab_size, EMBED_DIM, name="emb")(tok_in)
    h = L.Conv1D(CONV_FILTERS, KERNEL_SIZE, activation="relu", name="conv")(x)

    # ---------- domain-specific adapters (per-domain) ----------
    # Adapter0
    a0 = L.TimeDistributed(L.Dense(ADAPTER_R, activation="relu"), name="adapter0_down")(h)
    a0 = L.TimeDistributed(L.Dense(CONV_FILTERS, activation=None), name="adapter0_up")(a0)
    # Adapter1
    a1 = L.TimeDistributed(L.Dense(ADAPTER_R, activation="relu"), name="adapter1_down")(h)
    a1 = L.TimeDistributed(L.Dense(CONV_FILTERS, activation=None), name="adapter1_up")(a1)

    domf3 = L.Lambda(lambda d: tf.cast(tf.reshape(d, (-1, 1, 1)), tf.float32), name="dom_float3")(dom_in)
    a = L.Lambda(lambda t: (1.0 - t[0]) * t[1] + t[0] * t[2], name="select_adapter")([domf3, a0, a1])

    h = L.Add(name="adapter_residual")([h, a])

    # ---------- gate (FiLM/SE-like): use domain embedding + stats ----------
    d = L.Embedding(input_dim=NUM_DOMAINS, output_dim=GATE_HIDDEN, name="dom_emb")(dom_in)
    d = L.Dense(GATE_HIDDEN, activation="relu", name="dom_proj1")(d)
    d = L.Dense(CONV_FILTERS, activation=None, name="dom_proj2")(d)
    d = L.Reshape((1, CONV_FILTERS), name="dom_reshape")(d)

    s = L.GlobalAveragePooling1D(name="gap")(h)
    s = L.Dense(CONV_FILTERS, activation=None, name="stat_proj")(s)
    s = L.Reshape((1, CONV_FILTERS), name="stat_reshape")(s)

    gate_preact = L.Add(name="gate_preact")([d, s])
    gate = L.Activation("sigmoid", name="gate_sigmoid")(gate_preact)
    h = L.Multiply(name="gated")([h, gate])

    pooled = L.GlobalMaxPooling1D(name="gmp")(h)
    z = L.Dense(DENSE_UNITS, activation="relu", name="shared_fc")(pooled)
    z = L.Dropout(DROPOUT, name="drop")(z)

    logit0 = L.Dense(1, activation=None, name="head0")(z)
    logit1 = L.Dense(1, activation=None, name="head1")(z)

    domf = L.Lambda(lambda d: tf.cast(tf.reshape(d, (-1, 1)), tf.float32), name="dom_float")(dom_in)
    logit = L.Lambda(lambda t: (1.0 - t[0]) * t[1] + t[0] * t[2], name="select_head")([domf, logit0, logit1])

    out = L.Activation("sigmoid", name="out")(logit)

    model = Model(inputs={"tokens": tok_in, "domain_id": dom_in}, outputs=out, name="DAEWC_CNN_MH")
    model.compile(optimizer=make_optimizer(LR_SOURCE), loss="binary_crossentropy")
    return model


# =========================================================
# 5. Trainable strategies
# =========================================================

def layer_by_name(model: Model, name: str) -> L.Layer:
    for layer in model.layers:
        if layer.name == name:
            return layer
    raise KeyError(f"Layer {name} not found")

def set_trainable_plain_source(model: Model):
    # train backbone + head0; head1 unused anyway
    for layer in model.layers:
        layer.trainable = True
    # optional: freeze head1 during source pretrain (not needed)
    # layer_by_name(model, "head1").trainable = False

def set_trainable_plain_target_full(model: Model):
    # freeze source head0; train backbone + head1
    for layer in model.layers:
        layer.trainable = True
    layer_by_name(model, "head0").trainable = False

def set_trainable_plain_target_backbone_frozen(model: Model):
    # train only head1
    for layer in model.layers:
        layer.trainable = False
    layer_by_name(model, "head1").trainable = True

def set_trainable_daeewc_source(model: Model):
    # Train all shared + domain0 modules.
    for layer in model.layers:
        layer.trainable = True
    # Optionally freeze domain1-specific adapter/head (unused anyway)
    # layer_by_name(model, "adapter1_down").trainable = False
    # layer_by_name(model, "adapter1_up").trainable = False
    # layer_by_name(model, "head1").trainable = False

def freeze_gate_mlp(model: Model):
    # freeze dom_proj and stat_proj weights so only domain embeddings + adapters adjust gating
    for lname in ["dom_proj1", "dom_proj2", "stat_proj"]:
        try:
            layer_by_name(model, lname).trainable = False
        except KeyError:
            pass

def set_trainable_adapter_only_target(model: Model):
    """
    Adapter-only baseline: freeze backbone, train ONLY target-domain adapter1 + dom_emb (row1 only) + head1.
    Also freeze gate MLP weights for stability (domain embedding row1 can still change).
    """
    # freeze everything
    for layer in model.layers:
        layer.trainable = False

    # unfreeze target domain-specific parts
    for lname in ["adapter1_down", "adapter1_up", "head1", "dom_emb"]:
        layer_by_name(model, lname).trainable = True

    # keep gate MLP frozen (dom_proj1/2/stat_proj already frozen)
    freeze_gate_mlp(model)

def set_trainable_daeewc_stage1(model: Model):
    # stage1 == adapter-only training
    set_trainable_adapter_only_target(model)

def set_trainable_daeewc_stage2(model: Model, unfreeze_backbone: Tuple[str, ...] = ("shared_fc",)):
    """
    stage2: unfreeze a SMALL subset of backbone to allow controlled adaptation:
      - typical: shared_fc only
      - optional: conv too
    """
    set_trainable_adapter_only_target(model)
    for lname in unfreeze_backbone:
        layer_by_name(model, lname).trainable = True


# =========================================================
# 6. Fisher + EWC wrapper
# =========================================================

def smooth_labels_binary(y: tf.Tensor, smooth: float) -> tf.Tensor:
    y = tf.cast(y, tf.float32)
    return y * (1.0 - smooth) + 0.5 * smooth

def fisher_var_list(model: Model, backbone_layer_names: Tuple[str, ...]) -> List[tf.Variable]:
    vars_ = []
    for lname in backbone_layer_names:
        layer = layer_by_name(model, lname)
        vars_.extend(layer.trainable_variables)
    return vars_

def snapshot_vars(var_list: List[tf.Variable]) -> Dict[str, tf.Tensor]:
    return {v.name: tf.identity(v) for v in var_list}

def compute_fisher_diagonal(
    model: Model,
    X_tokens: np.ndarray,
    y: np.ndarray,
    domain_arr: np.ndarray,
    var_list: List[tf.Variable],
    max_samples: int,
    batch_size: int,
    label_smooth: float,
    training_true: bool,
    seed: int,
) -> Tuple[Dict[str, tf.Tensor], float, float]:
    """
    Returns fisher_dict, fisher_mean, elapsed_seconds
    """
    rng = np.random.RandomState(seed)
    n_total = len(y)
    n = min(n_total, int(max_samples))
    idx = rng.choice(n_total, size=n, replace=False)

    Xt = X_tokens[idx]
    dt = domain_arr[idx].astype(np.int32)
    yt = y[idx].astype(np.float32).reshape(-1, 1)

    ds = tf.data.Dataset.from_tensor_slices(({"tokens": Xt, "domain_id": dt}, yt)).batch(batch_size)

    fisher = {v.name: tf.zeros_like(v) for v in var_list}
    bce = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction="none")
    total = tf.constant(0, dtype=tf.int32)

    t0 = time.time()
    for x_b, y_b in ds:
        y_b = tf.cast(y_b, tf.float32)
        y_b = tf.reshape(y_b, (-1, 1))
        y_s = smooth_labels_binary(y_b, label_smooth)

        with tf.GradientTape() as tape:
            y_pred = model(x_b, training=training_true)
            nll = tf.reduce_sum(bce(y_s, y_pred))  # sum

        grads = tape.gradient(nll, var_list)
        bs = tf.shape(y_b)[0]
        total += bs

        for v, g in zip(var_list, grads):
            if g is not None:
                fisher[v.name] = fisher[v.name] + tf.square(g)

    total_f = tf.cast(total, next(iter(fisher.values())).dtype)
    for name in fisher:
        fisher[name] = fisher[name] / (total_f + 1e-12)

    fisher_mean = float(np.mean([float(tf.reduce_mean(v).numpy()) for v in fisher.values()])) if fisher else 0.0
    t_elapsed = time.time() - t0
    return fisher, fisher_mean, t_elapsed

class CLWrapper(tf.keras.Model):
    """
    A wrapper that:
      - adds backbone-only EWC + optional proximity
      - scales backbone gradients by backbone_lr_mult (to keep updates gentle)
    """
    def __init__(
        self,
        base_model: Model,
        fisher: Optional[Dict[str, tf.Tensor]],
        theta_old: Optional[Dict[str, tf.Tensor]],
        ewc_lambda: float,
        prox_alpha: float,
        backbone_var_names: Optional[set],
        backbone_lr_mult: float,
    ):
        super().__init__()
        self.base = base_model
        self.fisher = fisher or {}
        self.theta_old = theta_old or {}
        self.ewc_lambda = float(ewc_lambda)
        self.prox_alpha = float(prox_alpha)
        self.backbone_var_names = backbone_var_names or set()
        self.backbone_lr_mult = float(backbone_lr_mult)

    def call(self, inputs, training=False):
        return self.base(inputs, training=training)

    def train_step(self, data):
        x, y = data
        y = tf.cast(y, tf.float32)
        y = tf.reshape(y, (-1, 1))

        vars_all = self.base.trainable_variables

        with tf.GradientTape() as tape:
            y_pred = self.base(x, training=True)
            loss = self.compiled_loss(y, y_pred, regularization_losses=self.base.losses)

            # EWC penalty
            if self.ewc_lambda > 0.0 and self.fisher:
                ewc_pen = tf.constant(0.0, dtype=loss.dtype)
                for v in vars_all:
                    if v.name in self.fisher:
                        delta = v - self.theta_old[v.name]
                        ewc_pen += tf.reduce_sum(self.fisher[v.name] * tf.square(delta))
                loss = loss + 0.5 * self.ewc_lambda * ewc_pen

            # Proximity penalty (uniform L2 to theta_old)
            if self.prox_alpha > 0.0 and self.theta_old:
                prox = tf.constant(0.0, dtype=loss.dtype)
                for v in vars_all:
                    if v.name in self.theta_old:
                        delta = v - self.theta_old[v.name]
                        prox += tf.reduce_sum(tf.square(delta))
                loss = loss + 0.5 * self.prox_alpha * prox

        grads = tape.gradient(loss, vars_all)

        # scale backbone grads
        if self.backbone_lr_mult != 1.0 and self.backbone_var_names:
            new_grads = []
            for g, v in zip(grads, vars_all):
                if g is None:
                    new_grads.append(None)
                    continue
                if v.name in self.backbone_var_names:
                    new_grads.append(g * self.backbone_lr_mult)
                else:
                    new_grads.append(g)
            grads = new_grads

        self.optimizer.apply_gradients(zip(grads, vars_all))
        return {"loss": loss}


# =========================================================
# 7. Load datasets
# =========================================================
print("Loading datasets...")

fake_df = pd.read_csv(PATH_FAKE)
real_df = pd.read_csv(PATH_REAL)
covid_fake_df = pd.read_csv(PATH_COVID_FAKE)
covid_real_df = pd.read_csv(PATH_COVID_REAL)

# Political (ISOT subset)
pol_fake = fake_df[(fake_df["subject"] == "politics") & (fake_df["text"].astype(str).str.len() >= MIN_CHAR_LEN)]["text"].astype(str)
pol_real = real_df[(real_df["subject"] == "politicsNews") & (real_df["text"].astype(str).str.len() >= MIN_CHAR_LEN)]["text"].astype(str)

pol_texts = [basic_clean(x) for x in pd.concat([pol_fake, pol_real]).tolist()]
pol_labels = np.concatenate([
    np.zeros(len(pol_fake), dtype=int),
    np.ones(len(pol_real), dtype=int),
])

print(f"Political | fake={len(pol_fake)}  real={len(pol_real)}  total={len(pol_texts)}")

# Medical (COVID dataset)
col_f = pick_text_col(covid_fake_df)
col_r = pick_text_col(covid_real_df)

med_fake = covid_fake_df[covid_fake_df[col_f].astype(str).str.len() >= MIN_CHAR_LEN][col_f].astype(str)
med_real = covid_real_df[covid_real_df[col_r].astype(str).str.len() >= MIN_CHAR_LEN][col_r].astype(str)

med_texts = [basic_clean(x) for x in pd.concat([med_fake, med_real]).tolist()]
med_labels = np.concatenate([
    np.zeros(len(med_fake), dtype=int),
    np.ones(len(med_real), dtype=int),
])

print(f"Medical   | fake={len(med_fake)}  real={len(med_real)}  total={len(med_texts)}")


# =========================================================
# 8. Split train/dev/test (fixed)
# =========================================================

X_pol_train_texts, X_pol_dev_texts, X_pol_test_texts, y_pol_train, y_pol_dev, y_pol_test = split_train_dev_test(
    pol_texts, pol_labels, seed=DATA_SPLIT_SEED, test_size=0.2, dev_size=0.1
)
X_med_train_texts, X_med_dev_texts, X_med_test_texts, y_med_train_full, y_med_dev, y_med_test = split_train_dev_test(
    med_texts, med_labels, seed=DATA_SPLIT_SEED, test_size=0.2, dev_size=0.1
)

print("\nSplit sizes:")
print(f"Political: train={len(X_pol_train_texts)}  dev={len(X_pol_dev_texts)}  test={len(X_pol_test_texts)}")
print(f"Medical  : train={len(X_med_train_texts)}  dev={len(X_med_dev_texts)}  test={len(X_med_test_texts)}")


# =========================================================
# 9. Tokenizer (source-only or union)
# =========================================================

if TOKENIZER_MODE == "source":
    tok_texts = X_pol_train_texts
    print("\nTokenizer mode: SOURCE-only (paper-strict)")
elif TOKENIZER_MODE == "union":
    tok_texts = X_pol_train_texts + X_med_train_texts
    print("\nTokenizer mode: UNION (source_train + target_train texts)")
else:
    raise ValueError("TOKENIZER_MODE must be 'source' or 'union'")

TOK, VOCAB_SIZE = build_tokenizer(tok_texts, MAX_NUM_WORDS)
print(f"Vocab size = {VOCAB_SIZE} (cap={MAX_NUM_WORDS})")

# Vectorize all splits once
X_pol_train = vectorize(TOK, X_pol_train_texts, MAXLEN)
X_pol_dev = vectorize(TOK, X_pol_dev_texts, MAXLEN)
X_pol_test = vectorize(TOK, X_pol_test_texts, MAXLEN)

X_med_train_full = vectorize(TOK, X_med_train_texts, MAXLEN)
X_med_dev = vectorize(TOK, X_med_dev_texts, MAXLEN)
X_med_test = vectorize(TOK, X_med_test_texts, MAXLEN)

D_pol_train = domain_ids(len(X_pol_train), 0)
D_pol_dev = domain_ids(len(X_pol_dev), 0)
D_pol_test = domain_ids(len(X_pol_test), 0)

D_med_dev = domain_ids(len(X_med_dev), 1)
D_med_test = domain_ids(len(X_med_test), 1)


# =========================================================
# 10. Experiment runners
# =========================================================

@dataclass
class PretrainArtifacts:
    weights_plain: List[np.ndarray]
    weights_adp: List[np.ndarray]
    fisher_plain: Dict[str, tf.Tensor]
    fisher_adp: Dict[str, tf.Tensor]
    theta_plain: Dict[str, tf.Tensor]
    theta_adp: Dict[str, tf.Tensor]
    fisher_mean_plain: float
    fisher_mean_adp: float
    thr_pol_plain: float
    thr_pol_adp: float
    time_pre_plain: float
    time_pre_adp: float
    time_fisher_plain: float
    time_fisher_adp: float
    backbone_var_names_plain: set
    backbone_var_names_adp: set

def pretrain_and_fisher(seed: int) -> PretrainArtifacts:
    """
    Pretrain plain + adapter models on POLITICAL (domain_id=0), compute backbone-only fisher.
    Returns weights + fisher + thresholds and times.
    """
    # -------- Plain pretrain --------
    clear_tf()
    reset_seeds(seed)
    print("\n[Pretrain Plain]")

    plain = build_plain_cnn_mh(VOCAB_SIZE)
    set_trainable_plain_source(plain)
    plain.compile(optimizer=make_optimizer(LR_SOURCE), loss="binary_crossentropy")

    t0 = time.time()
    plain.fit(
        {"tokens": X_pol_train, "domain_id": D_pol_train},
        y_pol_train,
        validation_data=({"tokens": X_pol_dev, "domain_id": D_pol_dev}, y_pol_dev),
        epochs=EPOCHS_SOURCE,
        batch_size=BATCH_SIZE,
        verbose=0,
    )
    time_pre_plain = time.time() - t0

    # Threshold on pol dev
    pol_dev_prob_plain = plain.predict({"tokens": X_pol_dev, "domain_id": D_pol_dev}, verbose=0).ravel()
    thr_pol_plain = select_threshold_macro_f1(y_pol_dev, pol_dev_prob_plain) if CALIBRATE_THR_ON_DEV else 0.5

    # Pretrain test metrics
    pol_test_prob_plain = plain.predict({"tokens": X_pol_test, "domain_id": D_pol_test}, verbose=0).ravel()
    met_pol_test_plain = evaluate_with_threshold(y_pol_test, pol_test_prob_plain, thr_pol_plain)
    pretty_print_metrics("Political TEST (Plain pretrain)", met_pol_test_plain)

    # Fisher (backbone only)
    fisher_plain, theta_plain, fisher_mean_plain, time_fisher_plain = {}, {}, 0.0, 0.0
    backbone_names_plain = {"emb", "conv", "shared_fc"}
    backbone_vars_plain = fisher_var_list(plain, tuple(backbone_names_plain))
    backbone_var_names_plain = set([v.name for v in backbone_vars_plain])

    if USE_EWC:
        fisher_plain, fisher_mean_plain, time_fisher_plain = compute_fisher_diagonal(
            model=plain,
            X_tokens=X_pol_train,
            y=y_pol_train,
            domain_arr=D_pol_train,
            var_list=backbone_vars_plain,
            max_samples=FISHER_SAMPLES,
            batch_size=FISHER_BATCH_SIZE,
            label_smooth=FISHER_LABEL_SMOOTH,
            training_true=FISHER_TRAINING_TRUE,
            seed=seed,
        )
        theta_plain = snapshot_vars(backbone_vars_plain)
        print(f"Plain Fisher mean={fisher_mean_plain:.6e}  fisher_time={time_fisher_plain:.2f}s")

    weights_plain = plain.get_weights()

    # -------- Adapter pretrain --------
    clear_tf()
    reset_seeds(seed)
    print("\n[Pretrain DAEWC-Backbone (Adapter model)]")

    adp = build_daeewc_cnn_mh(VOCAB_SIZE)
    set_trainable_daeewc_source(adp)
    adp.compile(optimizer=make_optimizer(LR_SOURCE), loss="binary_crossentropy")

    t0 = time.time()
    adp.fit(
        {"tokens": X_pol_train, "domain_id": D_pol_train},
        y_pol_train,
        validation_data=({"tokens": X_pol_dev, "domain_id": D_pol_dev}, y_pol_dev),
        epochs=EPOCHS_SOURCE,
        batch_size=BATCH_SIZE,
        verbose=0,
    )
    time_pre_adp = time.time() - t0

    pol_dev_prob_adp = adp.predict({"tokens": X_pol_dev, "domain_id": D_pol_dev}, verbose=0).ravel()
    thr_pol_adp = select_threshold_macro_f1(y_pol_dev, pol_dev_prob_adp) if CALIBRATE_THR_ON_DEV else 0.5

    pol_test_prob_adp = adp.predict({"tokens": X_pol_test, "domain_id": D_pol_test}, verbose=0).ravel()
    met_pol_test_adp = evaluate_with_threshold(y_pol_test, pol_test_prob_adp, thr_pol_adp)
    pretty_print_metrics("Political TEST (Adapter pretrain)", met_pol_test_adp)

    # Fisher on adapter backbone vars
    fisher_adp, theta_adp, fisher_mean_adp, time_fisher_adp = {}, {}, 0.0, 0.0
    backbone_names_adp = {"emb", "conv", "shared_fc"}
    backbone_vars_adp = fisher_var_list(adp, tuple(backbone_names_adp))
    backbone_var_names_adp = set([v.name for v in backbone_vars_adp])

    if USE_EWC:
        fisher_adp, fisher_mean_adp, time_fisher_adp = compute_fisher_diagonal(
            model=adp,
            X_tokens=X_pol_train,
            y=y_pol_train,
            domain_arr=D_pol_train,
            var_list=backbone_vars_adp,
            max_samples=FISHER_SAMPLES,
            batch_size=FISHER_BATCH_SIZE,
            label_smooth=FISHER_LABEL_SMOOTH,
            training_true=FISHER_TRAINING_TRUE,
            seed=seed,
        )
        theta_adp = snapshot_vars(backbone_vars_adp)
        print(f"Adapter Fisher mean={fisher_mean_adp:.6e}  fisher_time={time_fisher_adp:.2f}s")

    weights_adp = adp.get_weights()

    return PretrainArtifacts(
        weights_plain=weights_plain,
        weights_adp=weights_adp,
        fisher_plain=fisher_plain,
        fisher_adp=fisher_adp,
        theta_plain=theta_plain,
        theta_adp=theta_adp,
        fisher_mean_plain=fisher_mean_plain,
        fisher_mean_adp=fisher_mean_adp,
        thr_pol_plain=thr_pol_plain,
        thr_pol_adp=thr_pol_adp,
        time_pre_plain=time_pre_plain,
        time_pre_adp=time_pre_adp,
        time_fisher_plain=time_fisher_plain,
        time_fisher_adp=time_fisher_adp,
        backbone_var_names_plain=backbone_var_names_plain,
        backbone_var_names_adp=backbone_var_names_adp,
    )

def auto_scale_lambda(base_lambda: float, mean_ref: float, mean_other: float) -> float:
    if not EWC_AUTO_SCALE or mean_ref <= 0 or mean_other <= 0:
        return base_lambda
    scale = (mean_ref + EWC_EPS) / (mean_other + EWC_EPS)
    return float(base_lambda * scale)

def eval_domain(
    model: Model,
    X_dev: np.ndarray, y_dev: np.ndarray, D_dev: np.ndarray,
    X_test: np.ndarray, y_test: np.ndarray, D_test: np.ndarray,
) -> Tuple[float, Dict[str, float], Dict[str, float]]:
    """
    Returns (thr_dev, metrics_on_dev, metrics_on_test) with threshold calibrated on DEV if enabled.
    """
    prob_dev = model.predict({"tokens": X_dev, "domain_id": D_dev}, verbose=0).ravel()
    if CALIBRATE_THR_ON_DEV:
        thr = select_threshold_macro_f1(y_dev, prob_dev)
    else:
        thr = 0.5

    met_dev = evaluate_with_threshold(y_dev, prob_dev, thr)
    prob_test = model.predict({"tokens": X_test, "domain_id": D_test}, verbose=0).ravel()
    met_test = evaluate_with_threshold(y_test, prob_test, thr)
    return thr, met_dev, met_test


# =========================================================
# 11. Run all experiments
# =========================================================

records: List[Dict] = []

def add_record(
    seed: int,
    shot_key: float,
    method: str,
    met_tgt_test: Dict[str, float],
    met_src_test_after: Dict[str, float],
    src_f1_pre: float,
    src_thr_pre: float,
    trainable_params: int,
    t_pretrain: float,
    t_fisher: float,
    t_adapt: float,
):
    forget_f1 = float(src_f1_pre - met_src_test_after["f1_macro"])
    avg_f1 = 0.5 * float(met_tgt_test["f1_macro"] + met_src_test_after["f1_macro"])
    records.append({
        "seed": seed,
        "shot": shot_key,
        "method": method,
        "tgt_acc": met_tgt_test["acc"],
        "tgt_f1_macro": met_tgt_test["f1_macro"],
        "tgt_bce": met_tgt_test["bce"],
        "src_thr_pre": src_thr_pre,
        "src_f1_pre": src_f1_pre,
        "src_acc_after": met_src_test_after["acc"],
        "src_f1_after": met_src_test_after["f1_macro"],
        "src_bce_after": met_src_test_after["bce"],
        "forget_f1": forget_f1,
        "avg_f1_after": avg_f1,
        "trainable_params": trainable_params,
        "t_pretrain": t_pretrain,
        "t_fisher": t_fisher,
        "t_adapt": t_adapt,
        "t_total_once": float(t_pretrain + t_fisher + t_adapt),
    })

def summarize_and_save(df: pd.DataFrame):
    df.to_csv(OUT_RAW_CSV, index=False)
    print(f"\nSaved raw: {OUT_RAW_CSV}")

    # summary mean + 95% CI over seeds
    def mean_ci(x: np.ndarray) -> Tuple[float, float]:
        x = np.asarray(x, dtype=float)
        n = len(x)
        if n <= 1:
            return float(np.mean(x)), 0.0
        m = float(np.mean(x))
        s = float(np.std(x, ddof=1))
        ci = 1.96 * s / np.sqrt(n)
        return m, ci

    rows = []
    group_cols = ["shot", "method"]
    for (shot, method), g in df.groupby(group_cols):
        m_f1, ci_f1 = mean_ci(g["tgt_f1_macro"].values)
        m_forget, ci_forget = mean_ci(g["forget_f1"].values)
        m_time, ci_time = mean_ci(g["t_adapt"].values)
        m_params = float(g["trainable_params"].iloc[0])  # deterministic per method
        rows.append({
            "shot": shot,
            "method": method,
            "tgt_f1_macro_mean": m_f1,
            "tgt_f1_macro_ci95": ci_f1,
            "forget_f1_mean": m_forget,
            "forget_f1_ci95": ci_forget,
            "adapt_time_mean_s": m_time,
            "adapt_time_ci95": ci_time,
            "trainable_params": m_params,
        })
    df_sum = pd.DataFrame(rows).sort_values(["shot", "method"])
    df_sum.to_csv(OUT_SUMMARY_CSV, index=False)
    print(f"Saved summary: {OUT_SUMMARY_CSV}")

    # quick pivots
    print("\n=== Pivot: Target Macro-F1 (mean) ===")
    print(df_sum.pivot(index="shot", columns="method", values="tgt_f1_macro_mean"))

    print("\n=== Pivot: Forgetting (ΔSrc Macro-F1, lower is better) ===")
    print(df_sum.pivot(index="shot", columns="method", values="forget_f1_mean"))

    print("\n=== Pivot: Adapt time (s, mean) ===")
    print(df_sum.pivot(index="shot", columns="method", values="adapt_time_mean_s"))


# Main loop
for seed in SEEDS:
    print("\n" + "#" * 120)
    print(f"RUN seed={seed}")
    reset_seeds(seed)

    # Pretrain once per seed (paper-style)
    art = pretrain_and_fisher(seed)

    # EWC lambda scaling
    lambda_plain = EWC_LAMBDA
    lambda_adp = auto_scale_lambda(EWC_LAMBDA, art.fisher_mean_plain, art.fisher_mean_adp)
    if USE_EWC and EWC_AUTO_SCALE:
        print(f"\nAuto-scaled lambda: plain={lambda_plain:.3f}  adapter={lambda_adp:.3f}")

    # Source "pre" score for forgetting (evaluate on source TEST using source threshold from DEV)
    # We'll compute pre source test F1 for each model type from the pretraining artifacts by rebuilding and predicting,
    # ensuring consistency of variable naming.
    # Plain pre
    clear_tf()
    reset_seeds(seed)
    plain_pre = build_plain_cnn_mh(VOCAB_SIZE)
    plain_pre.set_weights(art.weights_plain)
    pol_test_prob_plain = plain_pre.predict({"tokens": X_pol_test, "domain_id": D_pol_test}, verbose=0).ravel()
    met_src_pre_plain = evaluate_with_threshold(y_pol_test, pol_test_prob_plain, art.thr_pol_plain)
    src_f1_pre_plain = met_src_pre_plain["f1_macro"]

    # Adapter pre
    clear_tf()
    reset_seeds(seed)
    adp_pre = build_daeewc_cnn_mh(VOCAB_SIZE)
    adp_pre.set_weights(art.weights_adp)
    pol_test_prob_adp = adp_pre.predict({"tokens": X_pol_test, "domain_id": D_pol_test}, verbose=0).ravel()
    met_src_pre_adp = evaluate_with_threshold(y_pol_test, pol_test_prob_adp, art.thr_pol_adp)
    src_f1_pre_adp = met_src_pre_adp["f1_macro"]

    # Determine shot keys
    if USE_FRACTIONS:
        shot_keys = FRACTIONS
    else:
        shot_keys = SHOTS_PER_CLASS_LIST

    for shot_key in shot_keys:
        # Build target few-shot subset from medical TRAIN
        if USE_FRACTIONS:
            frac = float(shot_key)
            X_med_sub_texts, y_med_sub = stratified_fraction_subset(X_med_train_texts, y_med_train_full, frac, seed=DATA_SPLIT_SEED)
            shot_desc = f"frac={frac:.2f}"
        else:
            k = int(shot_key)
            # you can vary sampling per seed if you want: seed=DATA_SPLIT_SEED + seed
            X_med_sub_texts, y_med_sub = sample_k_shot_per_class(X_med_train_texts, y_med_train_full, k, seed=DATA_SPLIT_SEED)
            shot_desc = f"{k}-shot/class"

        X_med_sub = vectorize(TOK, X_med_sub_texts, MAXLEN)
        D_med_sub = domain_ids(len(X_med_sub), 1)

        print("\n" + "-" * 110)
        print(f"Target setting: {shot_desc}  (n={len(X_med_sub)})")

        # =========================================================
        # Baseline: From-scratch on target (Plain)
        # =========================================================
        if INCLUDE_SCRATCH:
            print("\n[Scratch-Plain]")
            clear_tf()
            reset_seeds(seed)

            m = build_plain_cnn_mh(VOCAB_SIZE)
            # train only head1 or full? For scratch we allow full training.
            for layer in m.layers:
                layer.trainable = True
            # but head0 unused; harmless
            m.compile(optimizer=make_optimizer(LR_TARGET), loss="binary_crossentropy")

            t_adapt = timed_fit(
                m,
                {"tokens": X_med_sub, "domain_id": D_med_sub}, y_med_sub,
                {"tokens": X_med_dev, "domain_id": D_med_dev}, y_med_dev,
                epochs=EPOCHS_SOURCE,
                batch_size=BATCH_SIZE,
                verbose=0,
            )

            # Evaluate target
            _, met_med_dev, met_med_test = eval_domain(m, X_med_dev, y_med_dev, D_med_dev, X_med_test, y_med_test, D_med_test)
            pretty_print_metrics("Medical TEST (Scratch-Plain)", met_med_test)

            # For scratch, no meaningful source forgetting. We still can evaluate, but it will be random.
            # We'll skip recording forgetting by setting src after = src pre (no forgetting) NOT correct; so skip.
            # Here we record with NaNs:
            records.append({
                "seed": seed,
                "shot": shot_key,
                "method": "Scratch-Plain",
                "tgt_acc": met_med_test["acc"],
                "tgt_f1_macro": met_med_test["f1_macro"],
                "tgt_bce": met_med_test["bce"],
                "src_thr_pre": np.nan,
                "src_f1_pre": np.nan,
                "src_acc_after": np.nan,
                "src_f1_after": np.nan,
                "src_bce_after": np.nan,
                "forget_f1": np.nan,
                "avg_f1_after": np.nan,
                "trainable_params": count_trainable_params(m),
                "t_pretrain": 0.0,
                "t_fisher": 0.0,
                "t_adapt": t_adapt,
                "t_total_once": t_adapt,
            })

        # =========================================================
        # Transfer: Plain Full FT (multi-head; head0 frozen)
        # =========================================================
        print("\n[Transfer-Plain FullFT]")
        clear_tf()
        reset_seeds(seed)

        m = build_plain_cnn_mh(VOCAB_SIZE)
        m.set_weights(art.weights_plain)
        set_trainable_plain_target_full(m)
        m.compile(optimizer=make_optimizer(LR_TARGET), loss="binary_crossentropy")

        t_adapt = timed_fit(
            m,
            {"tokens": X_med_sub, "domain_id": D_med_sub}, y_med_sub,
            {"tokens": X_med_dev, "domain_id": D_med_dev}, y_med_dev,
            epochs=EPOCHS_SOURCE,
            batch_size=BATCH_SIZE,
            verbose=0,
        )

        # Target metrics
        _, met_med_dev, met_med_test = eval_domain(m, X_med_dev, y_med_dev, D_med_dev, X_med_test, y_med_test, D_med_test)
        pretty_print_metrics("Medical TEST (Transfer-Plain)", met_med_test)

        # Source AFTER metrics on POL TEST using PRETRAIN threshold (fixed)
        pol_test_prob_after = m.predict({"tokens": X_pol_test, "domain_id": D_pol_test}, verbose=0).ravel()
        met_src_after = evaluate_with_threshold(y_pol_test, pol_test_prob_after, art.thr_pol_plain)
        pretty_print_metrics("Political TEST after (Transfer-Plain)", met_src_after)

        add_record(
            seed=seed,
            shot_key=shot_key,
            method="Transfer-Plain",
            met_tgt_test=met_med_test,
            met_src_test_after=met_src_after,
            src_f1_pre=src_f1_pre_plain,
            src_thr_pre=art.thr_pol_plain,
            trainable_params=count_trainable_params(m),
            t_pretrain=art.time_pre_plain,
            t_fisher=0.0,
            t_adapt=t_adapt,
        )

        # =========================================================
        # Transfer: Plain + EWC (backbone-only)
        # =========================================================
        if INCLUDE_PLAIN_EWC and USE_EWC:
            print("\n[Transfer-Plain + EWC]")
            clear_tf()
            reset_seeds(seed)

            base = build_plain_cnn_mh(VOCAB_SIZE)
            base.set_weights(art.weights_plain)
            set_trainable_plain_target_full(base)

            wrapper = CLWrapper(
                base_model=base,
                fisher=art.fisher_plain,
                theta_old=art.theta_plain,
                ewc_lambda=lambda_plain,
                prox_alpha=PROX_ALPHA,
                backbone_var_names=art.backbone_var_names_plain,
                backbone_lr_mult=1.0,  # Full FT style (no grad scaling)
            )
            wrapper.compile(optimizer=make_optimizer(LR_TARGET), loss="binary_crossentropy")

            t_adapt = timed_fit(
                wrapper,
                {"tokens": X_med_sub, "domain_id": D_med_sub}, y_med_sub,
                {"tokens": X_med_dev, "domain_id": D_med_dev}, y_med_dev,
                epochs=EPOCHS_SOURCE,
                batch_size=BATCH_SIZE,
                verbose=0,
            )

            # Target
            _, met_med_dev, met_med_test = eval_domain(wrapper, X_med_dev, y_med_dev, D_med_dev, X_med_test, y_med_test, D_med_test)
            pretty_print_metrics("Medical TEST (Plain+EWC)", met_med_test)

            # Source after (fixed src threshold)
            pol_test_prob_after = wrapper.predict({"tokens": X_pol_test, "domain_id": D_pol_test}, verbose=0).ravel()
            met_src_after = evaluate_with_threshold(y_pol_test, pol_test_prob_after, art.thr_pol_plain)
            pretty_print_metrics("Political TEST after (Plain+EWC)", met_src_after)

            add_record(
                seed=seed,
                shot_key=shot_key,
                method="Transfer-Plain+EWC",
                met_tgt_test=met_med_test,
                met_src_test_after=met_src_after,
                src_f1_pre=src_f1_pre_plain,
                src_thr_pre=art.thr_pol_plain,
                trainable_params=count_trainable_params(base),
                t_pretrain=art.time_pre_plain,
                t_fisher=art.time_fisher_plain,
                t_adapt=t_adapt,
            )

        # =========================================================
        # Adapter-only baseline (DAEWC model; backbone frozen)
        # =========================================================
        if INCLUDE_ADAPTER_ONLY:
            print("\n[Adapter-Only (DAEWC model; backbone frozen)]")
            clear_tf()
            reset_seeds(seed)

            adp = build_daeewc_cnn_mh(VOCAB_SIZE)
            adp.set_weights(art.weights_adp)
            set_trainable_adapter_only_target(adp)
            adp.compile(optimizer=make_optimizer(LR_TARGET), loss="binary_crossentropy")

            t_adapt = timed_fit(
                adp,
                {"tokens": X_med_sub, "domain_id": D_med_sub}, y_med_sub,
                {"tokens": X_med_dev, "domain_id": D_med_dev}, y_med_dev,
                epochs=EPOCHS_SOURCE,
                batch_size=BATCH_SIZE,
                verbose=0,
            )

            # Target
            _, met_med_dev, met_med_test = eval_domain(adp, X_med_dev, y_med_dev, D_med_dev, X_med_test, y_med_test, D_med_test)
            pretty_print_metrics("Medical TEST (Adapter-Only)", met_med_test)

            # Source after (fixed src threshold)
            pol_test_prob_after = adp.predict({"tokens": X_pol_test, "domain_id": D_pol_test}, verbose=0).ravel()
            met_src_after = evaluate_with_threshold(y_pol_test, pol_test_prob_after, art.thr_pol_adp)
            pretty_print_metrics("Political TEST after (Adapter-Only)", met_src_after)

            add_record(
                seed=seed,
                shot_key=shot_key,
                method="Adapter-Only",
                met_tgt_test=met_med_test,
                met_src_test_after=met_src_after,
                src_f1_pre=src_f1_pre_adp,
                src_thr_pre=art.thr_pol_adp,
                trainable_params=count_trainable_params(adp),
                t_pretrain=art.time_pre_adp,
                t_fisher=0.0,
                t_adapt=t_adapt,
            )

        # =========================================================
        # DAEWC (Stage1 adapters; Stage2 small backbone + EWC + grad scaling)
        # =========================================================
        if INCLUDE_DAEWC and USE_EWC:
            print("\n[DAEWC (Stage1 + Stage2)]")
            clear_tf()
            reset_seeds(seed)

            base = build_daeewc_cnn_mh(VOCAB_SIZE)
            base.set_weights(art.weights_adp)

            # ---- Stage1: adapters/head only ----
            set_trainable_daeewc_stage1(base)
            base.compile(optimizer=make_optimizer(LR_TARGET), loss="binary_crossentropy")

            t_stage1 = timed_fit(
                base,
                {"tokens": X_med_sub, "domain_id": D_med_sub}, y_med_sub,
                {"tokens": X_med_dev, "domain_id": D_med_dev}, y_med_dev,
                epochs=EPOCHS_TARGET_STAGE1,
                batch_size=BATCH_SIZE,
                verbose=0,
            )

            # ---- Stage2: unfreeze small backbone and wrap with EWC + grad scaling ----
            set_trainable_daeewc_stage2(base, unfreeze_backbone=("conv", "shared_fc"))
            wrapper = CLWrapper(
                base_model=base,
                fisher=art.fisher_adp,
                theta_old=art.theta_adp,
                ewc_lambda=lambda_adp,
                prox_alpha=PROX_ALPHA,
                backbone_var_names=art.backbone_var_names_adp,
                backbone_lr_mult=BACKBONE_LR_MULT,
            )
            wrapper.compile(optimizer=make_optimizer(LR_TARGET), loss="binary_crossentropy")

            t_stage2 = timed_fit(
                wrapper,
                {"tokens": X_med_sub, "domain_id": D_med_sub}, y_med_sub,
                {"tokens": X_med_dev, "domain_id": D_med_dev}, y_med_dev,
                epochs=EPOCHS_TARGET_STAGE2,
                batch_size=BATCH_SIZE,
                verbose=0,
            )

            t_adapt = t_stage1 + t_stage2

            # Target
            _, met_med_dev, met_med_test = eval_domain(wrapper, X_med_dev, y_med_dev, D_med_dev, X_med_test, y_med_test, D_med_test)
            pretty_print_metrics("Medical TEST (DAEWC)", met_med_test)

            # Source after (fixed src threshold)
            pol_test_prob_after = wrapper.predict({"tokens": X_pol_test, "domain_id": D_pol_test}, verbose=0).ravel()
            met_src_after = evaluate_with_threshold(y_pol_test, pol_test_prob_after, art.thr_pol_adp)
            pretty_print_metrics("Political TEST after (DAEWC)", met_src_after)

            add_record(
                seed=seed,
                shot_key=shot_key,
                method="DAEWC",
                met_tgt_test=met_med_test,
                met_src_test_after=met_src_after,
                src_f1_pre=src_f1_pre_adp,
                src_thr_pre=art.thr_pol_adp,
                trainable_params=count_trainable_params(base),
                t_pretrain=art.time_pre_adp,
                t_fisher=art.time_fisher_adp,
                t_adapt=t_adapt,
            )

        # =========================================================
        # Joint replay upper bound (not continual, but shows ceiling)
        # =========================================================
        if INCLUDE_JOINT_REPLAY_UPPER:
            print("\n[Replay Upper (Joint training)]")
            clear_tf()
            reset_seeds(seed)

            joint = build_plain_cnn_mh(VOCAB_SIZE)
            # train on pol+med, but need domain_id in batch:
            # We concatenate and feed domain id accordingly.
            X_joint = np.concatenate([X_pol_train, X_med_sub], axis=0)
            y_joint = np.concatenate([y_pol_train, y_med_sub], axis=0)
            D_joint = np.concatenate([D_pol_train, D_med_sub], axis=0)

            for layer in joint.layers:
                layer.trainable = True
            joint.compile(optimizer=make_optimizer(LR_TARGET), loss="binary_crossentropy")

            t_adapt = timed_fit(
                joint,
                {"tokens": X_joint, "domain_id": D_joint}, y_joint,
                {"tokens": X_med_dev, "domain_id": D_med_dev}, y_med_dev,
                epochs=EPOCHS_SOURCE,
                batch_size=BATCH_SIZE,
                verbose=0,
            )

            # Target
            _, met_med_dev, met_med_test = eval_domain(joint, X_med_dev, y_med_dev, D_med_dev, X_med_test, y_med_test, D_med_test)
            pretty_print_metrics("Medical TEST (Replay Upper)", met_med_test)

            # Source after: we can calibrate threshold on pol dev for this joint model,
            # but to keep consistent, we use the plain pretrain threshold (art.thr_pol_plain).
            pol_test_prob_after = joint.predict({"tokens": X_pol_test, "domain_id": D_pol_test}, verbose=0).ravel()
            met_src_after = evaluate_with_threshold(y_pol_test, pol_test_prob_after, art.thr_pol_plain)
            pretty_print_metrics("Political TEST (Replay Upper)", met_src_after)

            add_record(
                seed=seed,
                shot_key=shot_key,
                method="ReplayUpper",
                met_tgt_test=met_med_test,
                met_src_test_after=met_src_after,
                src_f1_pre=src_f1_pre_plain,   # not meaningful for joint, but keep for plotting
                src_thr_pre=art.thr_pol_plain,
                trainable_params=count_trainable_params(joint),
                t_pretrain=0.0,
                t_fisher=0.0,
                t_adapt=t_adapt,
            )

# Save + summarize
df = pd.DataFrame(records)
summarize_and_save(df)
print("\nDone.")